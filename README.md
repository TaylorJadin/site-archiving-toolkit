# Site Archiving Toolkit

## What is this thing?

This site archiving toolkit allows you to quickly and easily make both flattened HTML and web archive versions of websites. These scripts are a relatively easy to use command line interface for crawling sites using both [HTTrack](https://www.httrack.com) and [Browsertrix Crawler](https://github.com/webrecorder/browsertrix-crawler) (from the [Webrecorder](https://webrecorder.net) project) in Docker.

### Features

- Crawl an entire site / domain for offline browsing or preservation 
- Crawls can run in the background after theyâ€™ve been started, even if you close your terminal
- Accepts multiple URLs at once, to queue up multiple crawl jobs (on Linux or macOS, this is not supported on the Windows version)
- Preview archived pages using a local web server
- Automatically creates zip files of the archives

### Requirements

- A server or Desktop/Laptop running Linux, macOS, or Windows
- [Docker](https://docs.docker.com/engine/install/)

## How do I use it?

The Site Archiving Toolkit is designed first to be run on Reclaim Cloud, but can also be used on any computer that runs the latest version of Docker.

### Using the Site Archiving Toolkit on Reclaim Cloud

### Using the Site Archiving Toolkit on macOS or Linux

### Using the Site Archiving Toolkit on Windows